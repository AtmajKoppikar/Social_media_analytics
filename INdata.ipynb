{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exp1 - typed what is sm, fb, Twitter (1 Para each)\n",
    "\n",
    "Exp2- handwritten (ps, how to collect data-scrap/crawl/parse)\n",
    "Aim : select specific sm platform (topic specific)\n",
    "Theory : Ps 2 pg theory\n",
    "Conclusion\n",
    "\n",
    "Exp3- Typed - data cleaning and storage/security\n",
    "Aim : study preprocessing and storage\n",
    "\n",
    "Exp4-Visualization- Handwritten (Bar/pie + graph) ismai different visualisations k baare mai likkho\n",
    "\n",
    "Exp5- typed analyze trend/opinion generate profit business model using timestamp \n",
    "\n",
    "Exp6- typed business model, run a clustering algo like k means (on node/edges) : Channel titles as vertices and category id as edges. Run clustering\n",
    "\n",
    "Exp7- handwritten develop dashboard (3 or more components) visual components like bar graph pie chart etc\n",
    "\n",
    "Exp8-Create content for your own company for its promotion like posts on social media or banners(posters) or\n",
    "Think about some campaign (make banner)\n",
    "\n",
    "exp9 written theory sentiment analysis on textual data - plotly\n",
    "\n",
    "10th typed explain 9th with business model\n",
    "Business model - Imagine that u are a smol scale retailer, u want to sell accessories....u want to know which company acc do we need to keep more. Promotions and sentiment analysis for comments. Channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppossed to run NLP preprocessing\n",
    "Graph visualization and data analysis\n",
    "Suppossed to explain x and y axis. Explain the distribution\n",
    "Need a graph to run clustering algorithms. Run it on tags. Exp 5\n",
    "Exp 6 : Run k means on the graph.\n",
    "Have to make a bannner for youtube channel.\n",
    "Have to do sentiment analysis of competitor.\n",
    "Theory in exp 10\n",
    "handwrittenin exp 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.8.2.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from wordcloud) (1.21.4)\n",
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from wordcloud) (3.5.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib->wordcloud) (4.28.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/atmajkoppikar/Library/Python/3.9/lib/python/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/atmajkoppikar/Library/Python/3.9/lib/python/site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/atmajkoppikar/Library/Python/3.9/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /usr/local/bin/pip: bad interpreter: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python: no such file or directory\n",
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (49.2.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (1.21.4)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/atmajkoppikar/Library/Python/3.9/lib/python/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/atmajkoppikar/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from jinja2->spacy) (2.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "zsh:1: /usr/local/bin/pip: bad interpreter: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python: no such file or directory\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk) (2023.3.23)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/__init__.py\", line 22, in <module>\n",
      "    from . import multiarray\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/multiarray.py\", line 12, in <module>\n",
      "    from . import overrides\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/overrides.py\", line 7, in <module>\n",
      "    from numpy.core._multiarray_umath import (\n",
      "ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 0x0002): tried: '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 188, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 147, in _get_module_details\n",
      "    return _get_module_details(pkg_main_name, error)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 111, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/spacy/errors.py\", line 2, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/spacy/compat.py\", line 3, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/thinc/__init__.py\", line 2, in <module>\n",
      "    import numpy\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/__init__.py\", line 150, in <module>\n",
      "    from . import core\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/__init__.py\", line 48, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: \n",
      "\n",
      "IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n",
      "\n",
      "Importing the numpy C-extensions failed. This error can happen for\n",
      "many reasons, often due to issues with your setup or how NumPy was\n",
      "installed.\n",
      "\n",
      "We have compiled some common reasons and troubleshooting tips at:\n",
      "\n",
      "    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n",
      "\n",
      "Please note and check the following:\n",
      "\n",
      "  * The Python version is: Python3.9 from \"/usr/local/bin/python3\"\n",
      "  * The NumPy version is: \"1.21.4\"\n",
      "\n",
      "and make sure that they are the versions you expect.\n",
      "Please carefully study the documentation linked above for further help.\n",
      "\n",
      "Original error was: dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 0x0002): tried: '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Goals of the analysis\n",
    "We want to answer questions like:\n",
    "* How many views do our trending videos have? Do most of them have a large number of views? Is having a large number of views required for a video to become trending?\n",
    "* The same questions above, but applied to likes and comment count instead of views.\n",
    "* Which video remained the most on the trendin-videos list?\n",
    "* How many trending videos contain a fully-capitalized word in their titles?\n",
    "* What are the lengths of trending video titles? Is this length related to the video becoming trendy?\n",
    "* How are views, likes, dislikes, comment count, title length, and other attributes correlate with (relate to) each other? How are they connected?\n",
    "* What are the most common words in trending video titles?\n",
    "* Which YouTube channels have the largest number of trending videos?\n",
    "* Which video category (e.g. Entertainment, Gaming, Comedy, etc.) has the largest number of trending videos?\n",
    "* When were trending videos published? On which days of the week? at which times of the day?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"/usr/local/bin/python3\"\n  * The NumPy version is: \"1.21.4\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 0x0002): tried: '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/__init__.py:22\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m multiarray\n\u001b[1;32m     23\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/multiarray.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m overrides\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _multiarray_umath\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/overrides.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtextwrap\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_multiarray_umath\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     add_docstring, implement_array_function, _get_implementing_args)\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_inspect\u001b[39;00m \u001b[39mimport\u001b[39;00m getargspec\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 0x0002): tried: '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/__init__.py:150\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39m# Allow distributors to run custom init code\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _distributor_init\n\u001b[0;32m--> 150\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m core\n\u001b[1;32m    151\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    152\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/__init__.py:48\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[39mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m\"\"\"\u001b[39m \u001b[39m%\u001b[39m (sys\u001b[39m.\u001b[39mversion_info[\u001b[39m0\u001b[39m], sys\u001b[39m.\u001b[39mversion_info[\u001b[39m1\u001b[39m], sys\u001b[39m.\u001b[39mexecutable,\n\u001b[1;32m     47\u001b[0m         __version__, exc)\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[1;32m     49\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[39mfor\u001b[39;00m envkey \u001b[39min\u001b[39;00m env_added:\n",
      "\u001b[0;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"/usr/local/bin/python3\"\n  * The NumPy version is: \"1.21.4\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 0x0002): tried: '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import wordcloud\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiding warnings for cleaner display\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuring some options\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "# If you want interactive plots, uncomment the next line\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"read\"></a>Reading the dataset\n",
    "Then we read the dataset file which is in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"INvideos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set some configuration options just for improving visualization graphs; nothing crucial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_COLORS = [\"#268bd2\", \"#0052CC\", \"#FF5722\", \"#b58900\", \"#003f5c\"]\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "sns.set(style=\"ticks\")\n",
    "plt.rc('figure', figsize=(8, 5), dpi=100)\n",
    "plt.rc('axes', labelpad=20, facecolor=\"#ffffff\", linewidth=0.4, grid=True, labelsize=14)\n",
    "plt.rc('patch', linewidth=0)\n",
    "plt.rc('xtick.major', width=0.2)\n",
    "plt.rc('ytick.major', width=0.2)\n",
    "plt.rc('grid', color='#9E9E9E', linewidth=0.4)\n",
    "plt.rc('font', family='Arial', weight='400', size=10)\n",
    "plt.rc('text', color='#282828')\n",
    "plt.rc('savefig', pad_inches=0.3, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"feel\"></a>Getting a feel of the dataset\n",
    "Let's get a feel of our dataset by displaying its first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see some information about our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are `40,949` entries in the dataset. We can see also that all columns in the dataset are complete (i.e. they have `40,949` non-null entries) except `description` column which has some `null` values; it only has `40,379` non-null values.\n",
    "\n",
    "## <a name=\"clean\"></a>Data cleaning - Exp 3\n",
    "The `description` column has some null values. These are some of the rows whose description values are null. We can see that null values are denoted by `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"description\"].apply(lambda x: pd.isna(x))].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to do some sort of data cleaning, and to get rid of those null values, we put an empty string in place of each null value in the `description` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"description\"] = df[\"description\"].fillna(value=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"description\"].apply(lambda x: x == \"\")].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Preprocessing : Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy import displacy\n",
    "# from spacy import tokenizer\n",
    "# nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to run named entity recognition on string columns\n",
    "# def apply_ner(text):\n",
    "#     count = 0\n",
    "#     doc = nlp(text)\n",
    "#     entities = []\n",
    "#     for ent in doc.ents:\n",
    "# #         if ent.text != \"\":\n",
    "# #             count +=1\n",
    "#         entities.append([(ent.text, ent.label_)])\n",
    "# #     percentage = count/40949\n",
    "#     return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = df[\"title\"].apply(apply_ner)\n",
    "# df[\"title_entities\"] = temp\n",
    "# # df[\"channel_title_entities\"], channel_title_percentage = df[\"channel_title\"].apply(apply_ner)\n",
    "# # df[\"tags_entities\"], tags_percentage = df[\"tags\"].apply(apply_ner)\n",
    "# # df[\"description_entities\"], description_percentage = df[\"description\"].apply(apply_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[\"title_entities\"])\n",
    "# # print(\"Percentage of NER:\", title_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"channel_title_entities\"] = df[\"channel_title\"].apply(apply_ner)\n",
    "# df[\"tags_entities\"] = df[\"tags\"].apply(apply_ner)\n",
    "# df[\"description_entities\"] = df[\"description\"].apply(apply_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[\"tags_entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[\"channel_title_entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[\"description_entities\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis - Exp 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"coll\"></a>Dataset collection years\n",
    "Let's see in which years the dataset was collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = df[\"trending_date\"].apply(lambda x: '20' + x[:2]).value_counts() \\\n",
    "            .to_frame().reset_index() \\\n",
    "            .rename(columns={\"index\": \"year\", \"trending_date\": \"No_of_videos\"})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "_ = sns.barplot(x=\"year\", y=\"No_of_videos\", data=cdf, \n",
    "                 ax=ax)\n",
    "_ = ax.set(xlabel=\"Year\", ylabel=\"No. of videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage\n",
    "df[\"trending_date\"].apply(lambda x: '20' + x[:2]).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset was collected in 2017 and 2018 with `77%` of it in 2018 and `23%` in 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"descn\"></a>Describtion of numerical columns\n",
    "Now, let's see some statistical information about the numerical columns of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note from the table above that \n",
    "- The average number of views of a trending video is `2,360,784`. The median value for the number of views is `681,861`, which means that half the trending videos have views that are less than that number, and the other half have views larger than that number\n",
    "- The average number of likes of a trending video is `74,266`, while the average number of dislikes is `3,711`. The \n",
    "- Average comment count is `8,446` while the median is `1,856`\n",
    "\n",
    "How useful are the observations above? Do they really represent the data? Let's examine more. \n",
    "\n",
    "### <a name=\"vh\"></a>Views histogram\n",
    "let's plot a [histogram](https://www.mathsisfun.com/data/histograms.html) for the `views` column to take a look at its distribution: to see how many videos have between `10` million and `20` million views, how many videos have between `20` million and `30` million views, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "_ = sns.distplot(df[\"views\"], kde=False, ax=ax)\n",
    "_ = ax.set(xlabel=\"Views\", ylabel=\"No. of videos\", xticks=np.arange(0, 2.4e8, 1e7))\n",
    "_ = ax.set_xlim(right=2.5e8)\n",
    "_ = plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the vast majority of trending videos have `5` million views or less. We get the `5` million number by calculating\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{0.1 \\times 10^8}{2} = 5 \\times 10^6\n",
    "\\end{align}\n",
    "\n",
    "Now let us plot the histogram just for videos with `25` million views or less to get a closer look at the distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "_ = sns.distplot(df[df[\"views\"] < 25e6][\"views\"], kde=False, \n",
    "                 color=PLOT_COLORS[4], hist_kws={'alpha': 1}, ax=ax)\n",
    "_ = ax.set(xlabel=\"Views\", ylabel=\"No. of videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that the majority of trending videos have `1` million views or less. Let's see the exact percentage of videos less than `1` million views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['views'] < 1e6]['views'].count() / df['views'].count() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it is around `60%`. Similarly, we can see that the percentage of videos with less than `1.5` million views is around `71%`, and that the percentage of videos with less than `5` million views is around `91%`.\n",
    "\n",
    "### <a name=\"lh\"></a>Likes histogram\n",
    "\n",
    "After `views`, we plot the histogram for `likes` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('figure.subplot', wspace=0.9)\n",
    "fig, ax = plt.subplots()\n",
    "_ = sns.distplot(df[\"likes\"], kde=False, \n",
    "                 color=PLOT_COLORS[4], hist_kws={'alpha': 1}, \n",
    "                 bins=np.linspace(0, 6e6, 61), ax=ax)\n",
    "_ = ax.set(xlabel=\"Likes\", ylabel=\"No. of videos\")\n",
    "_ = plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the vast majority of trending videos have between `0` and `100,000` likes. Let us plot the histogram just for videos with `1000,000` likes or less to get a closer look at the distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "_ = sns.distplot(df[df[\"likes\"] <= 1e5][\"likes\"], kde=False, \n",
    "                 color=PLOT_COLORS[4], hist_kws={'alpha': 1}, ax=ax)\n",
    "_ = ax.set(xlabel=\"Likes\", ylabel=\"No. of videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the majority of trending videos have `40000` likes or less with a peak for videos with `2000` likes or less. \n",
    "\n",
    "Let's see the exact percentage of videos with less than `40000` likes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['likes'] < 4e4]['likes'].count() / df['likes'].count() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can see that the percentage of videos with less than `100,000` likes is around `84%`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"ch\"></a>Comment count histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "_ = sns.distplot(df[\"comment_count\"], kde=False, rug=False, \n",
    "                 color=PLOT_COLORS[4], hist_kws={'alpha': 1}, ax=ax)\n",
    "_ = ax.set(xlabel=\"Comment Count\", ylabel=\"No. of videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a closer look by eliminating entries with comment count larger than `200000` comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "_ = sns.distplot(df[df[\"comment_count\"] < 200000][\"comment_count\"], kde=False, rug=False, \n",
    "                 color=PLOT_COLORS[4], hist_kws={'alpha': 1}, \n",
    "                 bins=np.linspace(0, 2e5, 49), ax=ax)\n",
    "_ = ax.set(xlabel=\"Comment Count\", ylabel=\"No. of videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most trending videos have around\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{25000}{6} \\approx 4166 \\text{ comments}\n",
    "\\end{align}\n",
    "\n",
    "since each division in the graph has six histogram bins. \n",
    "\n",
    "As with views and likes, let's see the exact percentage of videos with less than `4000` comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['comment_count'] < 4000]['comment_count'].count() / df['comment_count'].count() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar way, we can see that the percentage of videos with less than `25,000` comments  is around `93%`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exp 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp based visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trending_date'] = pd.to_datetime(df['trending_date'], format='%y.%d.%m')\n",
    "\n",
    "daily_avg = df.groupby('trending_date')[['views', 'likes', 'dislikes', 'comment_count', 'tags']].mean()\n",
    "\n",
    "daily_avg.plot()\n",
    "\n",
    "daily_pct_change = daily_avg.pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"pub\"></a>Trending videos and their publishing time\n",
    "An example value of the `publish_time` column in our dataset is `2017-11-13T17:13:01.000Z`. And according to information on this page: https://www.w3.org/TR/NOTE-datetime, this means that the date of publishing the video is `2017-11-13` and the time is `17:13:01` in Coordinated Universal Time (UTC) time zone.\n",
    "\n",
    "Let's add two columns to represent the date and hour of publishing each video, then delete the original `publish_time` column because we will not need it anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"publishing_day\"] = df[\"publish_time\"].apply(\n",
    "    lambda x: datetime.datetime.strptime(x[:10], \"%Y-%m-%d\").date().strftime('%a'))\n",
    "df[\"publishing_hour\"] = df[\"publish_time\"].apply(lambda x: x[11:13])\n",
    "df.drop(labels='publish_time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"descnn\"></a>Description on non-numerical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = ['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we can see that there are `205` unique dates, which means that our dataset contains collected data about trending videos over **`205`** days.\n",
    "\n",
    "From `video_id` description, we can see that there are `40949` videos (which is expected because our dataset contains `40949` entries), but we can see also that there are only `6351` unique videos which means that some videos appeared on the trending videos list **on more than one day**.\n",
    "The table also tells us that the top frequent title is `WE MADE OUR MOM CRY...HER DREAM CAME TRUE!` and that it appeared `30` times on the trending videos list.\n",
    "\n",
    "But there is something strange in the description table above: Because there are `6351` unique video IDs, we expect to have `6351` unique video titles also, because we assume that each ID is linked to a corresponding title. One possible interpretation is that a trending video had some title when it appeared on the trending list, then it appeared again on another day but with a modified title. Similar explaination applies for `description` column as well.\n",
    "For `publish_time` column, the unique values are less than `6351`, but there is nothing strange here, because two different videos may be published at the same time.\n",
    "\n",
    "To verify our interpretation for `title` column, let's take a look at an example where a trending video appeared more than once on the trending list but with different titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"video_id\")\n",
    "groups = []\n",
    "wanted_groups = []\n",
    "for key, item in grouped:\n",
    "    groups.append(grouped.get_group(key))\n",
    "\n",
    "for g in groups:\n",
    "    if len(g['title'].unique()) != 1:\n",
    "        wanted_groups.append(g)\n",
    "\n",
    "wanted_groups[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this video appeared on the list with two different titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see which days of the week had the largest numbers of trending videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = df[\"publishing_day\"].value_counts()\\\n",
    "        .to_frame().reset_index().rename(columns={\"index\": \"publishing_day\", \"publishing_day\": \"No_of_videos\"})\n",
    "fig, ax = plt.subplots()\n",
    "_ = sns.barplot(x=\"publishing_day\", y=\"No_of_videos\", data=cdf, \n",
    "                palette=sns.color_palette(['#003f5c', '#374c80', '#7a5195', \n",
    "                                           '#bc5090', '#ef5675', '#ff764a', '#ffa600'], n_colors=7), ax=ax)\n",
    "_ = ax.set(xlabel=\"Publishing Day\", ylabel=\"No. of videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of trending videos published on Sunday and Saturday are noticeably less than the number of trending videos published on other days of the week.\n",
    "\n",
    "Now let's use `publishing_hour` column to see which publishing hours had the largest number of trending videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = df[\"publishing_hour\"].value_counts().to_frame().reset_index()\\\n",
    "        .rename(columns={\"index\": \"publishing_hour\", \"publishing_hour\": \"No_of_videos\"})\n",
    "fig, ax = plt.subplots()\n",
    "_ = sns.barplot(x=\"publishing_hour\", y=\"No_of_videos\", data=cdf, \n",
    "                palette=sns.cubehelix_palette(n_colors=24), ax=ax)\n",
    "_ = ax.set(xlabel=\"Publishing Hour\", ylabel=\"No. of videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the period between 2PM and 7PM, peaking between 4PM and 5PM, had the largest number of trending videos. We notice also that the period between 12AM and 1PM has the smallest number of trending videos. But why is that? Is it because people publish a lot more videos between 2PM and 7PM? Is it because how YouTube algorithm chooses trending videos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"cap\"></a>How many trending video titles contain capitalized word?\n",
    "Now we want to see how many trending video titles contain at least a capitalized word (e.g. HOW). To do that, we will add a new variable (column) to the dataset whose value is `True` if the video title has at least a capitalized word in it, and `False` otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "value_counts = df[\"contains_capitalized\"].value_counts().to_dict()\n",
    "fig, ax = plt.subplots()\n",
    "_ = ax.pie([value_counts[False], value_counts[True]], labels=['No', 'Yes'], \n",
    "           colors=['#003f5c', '#ffa600'], textprops={'color': '#040204'}, startangle=45)\n",
    "_ = ax.axis('equal')\n",
    "_ = ax.set_title('Title Contains Capitalized Word?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"contains_capitalized\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 44% of trending video titles contain at least a capitalized word. We will later use this added new column `contains_capitalized` in analyzing correlation between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"len\"></a>Video title lengths\n",
    "Let's add another column to our dataset to represent the length of each video title, then plot the histogram of title length to get an idea about the lengths of trnding video titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_length\"] = df[\"title\"].apply(lambda x: len(x))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "_ = sns.distplot(df[\"title_length\"], kde=False, rug=False, \n",
    "                 color=PLOT_COLORS[4], hist_kws={'alpha': 1}, ax=ax)\n",
    "_ = ax.set(xlabel=\"Title Length\", ylabel=\"No. of videos\", xticks=range(0, 110, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that title-length distribution resembles a normal distribution, where most videos have title lengths between 30 and 60 character approximately.\n",
    "\n",
    "Now let's draw a [scatter plot](https://www.mathsisfun.com/data/scatter-xy-plots.html) between title length and number of views to see the relationship between these two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "_ = ax.scatter(x=df['views'], y=df['title_length'], color=PLOT_COLORS[2], edgecolors=\"#000000\", linewidths=0.5)\n",
    "_ = ax.set(xlabel=\"Views\", ylabel=\"Title Length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the scatter plot, we can say that there is no relationship between the title length and the number of views. However, we notice an interesting thing: videos that have `100,000,000` views and more have title length between `33` and `55` characters approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "We can conclude that changing the title and thumbnail of a video over time to suit current buzzwords can help in generating views. Moreover, engagement can increase over time and a video may take a variable amount of time to reach the trending status. this can range from a few days to a few months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"corr\"></a>Correlation between dataset variables\n",
    "Now let's see how the dataset variables are [correlated](https://www.mathsisfun.com/data/correlation.html) with each other: for example, we would like to see how views and likes are correlated, meaning do views and likes increase and decrease together (positive correlation)? Does one of them increase when the other decrease and vice versa (negative correlation)? Or are they not correlated?\n",
    "\n",
    "Correlation is represented as a value between `-1` and `+1` where `+1` denotes the highest positive correlation, `-1` denotes the highest negative correlation, and `0` denotes that there is no correlation.\n",
    "\n",
    "Let's see the correlation table between our dataset variables (numerical and boolean variables only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see for example that views and likes are highly positively correlated with a correlation value of `0.85`; we see also a high positive correlation (`0.80`) between likes and comment count, and between dislikes and comment count (`0.70`). \n",
    "\n",
    "There is some positive correlation between views and dislikes, between views and comment count, between likes and dislikes.\n",
    "\n",
    "Now let's visualize the correlation table above using a [heatmap](https://www.wikiwand.com/en/Heat_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_labels = [x.replace('_', ' ').title() for x in \n",
    "            list(df.select_dtypes(include=['number', 'bool']).columns.values)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "_ = sns.heatmap(df.corr(), annot=True, xticklabels=h_labels, yticklabels=h_labels, cmap=sns.cubehelix_palette(as_cmap=True), ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation map and correlation table above say that views and likes are highly positively correlated. Let's verify that by plotting a scatter plot between views and likes to visualize the relationship between these variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "_ = plt.scatter(x=df['views'], y=df['likes'], color=PLOT_COLORS[2], edgecolors=\"#000000\", linewidths=0.5)\n",
    "_ = ax.set(xlabel=\"Views\", ylabel=\"Likes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that views and likes are truly positively correlated: as one increases, the other increases too—mostly.\n",
    "\n",
    "Another verification of the correlation matrix and map is the scatter plot we drew above between views and title length as it shows that there is no correlation between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"commti\"></a>Most common words in video titles\n",
    "Let's see if there are some words that are used significantly in trending video titles. We will display the `25` most common words in all trending video titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_words = list(df[\"title\"].apply(lambda x: x.split()))\n",
    "title_words = [x for y in title_words for x in y]\n",
    "Counter(title_words).most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring words like \"the\" and \"of\", we can see that \"-\" and \"|\" symbols occured a lot in the `40949` trending video titles: `11452` times and `10663` times respectively. We notice also that words \"Video\", \"Trailer\", \"How\", and \"2018\" are common in trending video titles; each occured in 1613-1901 video titles.\n",
    "\n",
    "Let's draw a word cloud for the titles of our trending videos, which is a way to visualize most common words in the titles; the more common the word is, the bigger its font size is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wc = wordcloud.WordCloud(width=1200, height=600, collocations=False, stopwords=None, background_color=\"white\", colormap=\"tab20b\").generate_from_frequencies(dict(Counter(title_words).most_common(150)))\n",
    "wc = wordcloud.WordCloud(width=1200, height=500, \n",
    "                         collocations=False, background_color=\"white\", \n",
    "                         colormap=\"tab20b\").generate(\" \".join(title_words))\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "_ = plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"chan\"></a>Which channels have the largest number of trending videos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = df.groupby(\"channel_title\").size().reset_index(name=\"video_count\") \\\n",
    "    .sort_values(\"video_count\", ascending=False).head(20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "_ = sns.barplot(x=\"video_count\", y=\"channel_title\", data=cdf,\n",
    "                palette=sns.cubehelix_palette(n_colors=20, reverse=True), ax=ax)\n",
    "_ = ax.set(xlabel=\"No. of videos\", ylabel=\"Channel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"cat\"></a>Which video category has the largest number of trending videos?\n",
    "First, we will add a column that contains category names based on the values in `category_id` column. We will use a category JSON file provided with the dataset which contains information about each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"US_category_id.json\") as f:\n",
    "    categories = json.load(f)[\"items\"]\n",
    "cat_dict = {}\n",
    "for cat in categories:\n",
    "    cat_dict[int(cat[\"id\"])] = cat[\"snippet\"][\"title\"]\n",
    "df['category_name'] = df['category_id'].map(cat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see which category had the largest number of trending videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = df[\"category_name\"].value_counts().to_frame().reset_index()\n",
    "cdf.rename(columns={\"index\": \"category_name\", \"category_name\": \"No_of_videos\"}, inplace=True)\n",
    "fig, ax = plt.subplots()\n",
    "_ = sns.barplot(x=\"category_name\", y=\"No_of_videos\", data=cdf, \n",
    "                palette=sns.cubehelix_palette(n_colors=16, reverse=True), ax=ax)\n",
    "_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "_ = ax.set(xlabel=\"Category\", ylabel=\"No. of videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Entertainment category contains the largest number of trending videos among other categories: around `10,000` videos, followed by Music category with around `6,200` videos, followed by Howto & Style category with around `4,100` videos, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_count = df['category_id'].value_counts()\n",
    "\n",
    "# Create bar chart\n",
    "plt.bar(category_count.index, category_count.values)\n",
    "plt.xticks(category_count.index)\n",
    "plt.xlabel('Category ID')\n",
    "plt.ylabel('Number of Videos')\n",
    "plt.title('Number of Trending Videos in Each Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"err\"></a>How many trending videos have an error?\n",
    "To see how many trending videos got removed or had some error, we can use `video_error_or_removed` column in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df[\"video_error_or_removed\"].value_counts().to_dict()\n",
    "fig, ax = plt.subplots()\n",
    "_ = ax.pie([value_counts[False], value_counts[True]], labels=['No', 'Yes'], \n",
    "        colors=['#003f5c', '#ffa600'], textprops={'color': '#040204'})\n",
    "_ = ax.axis('equal')\n",
    "_ = ax.set_title('Video Error or Removed?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"video_error_or_removed\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that out of videos that appeared on trending list (`40949` videos), there is a tiny portion (`23` videos) with errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"commdis\"></a>How many trending videos have their commets disabled?\n",
    "To know that, we use `comments_disabled` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df[\"comments_disabled\"].value_counts().to_dict()\n",
    "fig, ax = plt.subplots()\n",
    "_ = ax.pie(x=[value_counts[False], value_counts[True]], labels=['No', 'Yes'], \n",
    "           colors=['#003f5c', '#ffa600'], textprops={'color': '#040204'})\n",
    "_ = ax.axis('equal')\n",
    "_ = ax.set_title('Comments Disabled?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"comments_disabled\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that only `2%` of trending videos prevented users from commenting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ratdis\"></a>How many trending videos have their ratings disabled?\n",
    "To know that, we use `ratings_disabled` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df[\"ratings_disabled\"].value_counts().to_dict()\n",
    "fig, ax = plt.subplots()\n",
    "_ = ax.pie([value_counts[False], value_counts[True]], labels=['No', 'Yes'], \n",
    "            colors=['#003f5c', '#ffa600'], textprops={'color': '#040204'})\n",
    "_ = ax.axis('equal')\n",
    "_ = ax.set_title('Ratings Disabled?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ratings_disabled\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that only `169` trending videos out of `40949` prevented users from commenting.\n",
    "\n",
    "## <a name=\"commratdis\"></a>How many videos have both comments and ratings disabled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[(df[\"comments_disabled\"] == True) & (df[\"ratings_disabled\"] == True)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are just `106` trending videos that have both comments and ratings disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"conc\"></a>Conclusions\n",
    "Here are the some of the results we extracted from the analysis:\n",
    "* We analyzed a dataset that contains information about YouTube trending videos for 205 days. The dataset was collected in 2017 and 2018. It contains **`40949`** video entry.\n",
    "* `71%` of trending videos have less than `1.5` million views, and **`91%`** have less than **`5`** million views.\n",
    "* `68%` of trending videos have less than `40,000` likes, and **`84%`** have less than **`100,000`** likes.\n",
    "* `67%` of trending videos have less than `4,000` comments, and **`93%`** have less than **`25,000`** comments.\n",
    "* Some videos may appear on the trending videos list on more than one day. Our dataset contains `40494` entries but not for `40494` unique videos but for `6351`unique videos.\n",
    "* Trending videos that have **`100,000,000`** views and more  have title length between `33` and `55` characters approximately.\n",
    "* The delimiters `-` and `|` were common in trending video titles.\n",
    "* The words 'Official', 'Video', 'Trailer', 'How', and '2018' were common also in trending video titles.\n",
    "* There is a strong positive correlation between the number of views and the number of likes of trending videos: As one of them increases, the other increases, and vice versa.\n",
    "* There is a strong positive correlation also between the number of likes and the number of comments, and a slightly weaker one between the number of dislikes and the number of comments.\n",
    "* The category that has the largest number of trending videos is **'Entertainment'** with `9,964` videos, followed by 'Music' category with `6,472` videos, followed by 'Howto & Style' category with `4146` videos.\n",
    "* On the opposite side, the category that has the smallest number of trending videos is 'Shows' with `57` videos, followed by 'Nonprofits & Activisim' with `57` videos, followed by 'Autos & Vehicles' with `384` videos.\n",
    "\n",
    "If you like this analysis, please consider to **upvote** this analysis at the top of this page.\n",
    "\n",
    "Follow me on [Twitter](https://twitter.com/ammar_cel) to know when I publish something new, or visit [my website and blog](http://ammar-alyousfi.com).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Don't do wordcloud. Important word frequency in youtube videos. What are the common videos which people are watching in certain time limit? what videos are people watching after another? Find out similar content which is produced in YouTube? Topic that is maximum ( Extract keywords and topics and show similarity based on tags or genres or descriptions. Nodes will be videos and edges will be the similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
